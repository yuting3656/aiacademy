{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cat_dog_df.csv')\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df.type)\n",
    "\n",
    "def df_to_data(df, picsize):\n",
    "    img_ls = []\n",
    "    label_ls = []\n",
    "\n",
    "    for file_name, label in tqdm_notebook(zip(df.file_name, df.type)):\n",
    "        img = cv2.imread(file_name, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (picsize, picsize))\n",
    "        img_ls.append(img)\n",
    "        \n",
    "        onehot = np.zeros(2)  \n",
    "        onehot[label] = 1\n",
    "        label_ls.append(onehot)\n",
    "\n",
    "    x_data = np.array(img_ls).reshape(len(df), -1)  # flatten image data\n",
    "    x_data = x_data / 255.  # normalization\n",
    "    y_data = np.array(label_ls)\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picsize = 64\n",
    "X_train, y_train = df_to_data(train_df, picsize)\n",
    "X_test, y_test = df_to_data(test_df, picsize)\n",
    "\n",
    "print('size of training data:', X_train.shape, y_train.shape)\n",
    "print('size of testing data:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Early stopping and checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope('placeholder'):\n",
    "    input_data = tf.placeholder(tf.float32, shape=[None, picsize*picsize], name='X')\n",
    "    y_true = tf.placeholder(tf.float32, shape=[None, 2], name='y')\n",
    "    \n",
    "with tf.variable_scope('network'):\n",
    "    h1 = tf.layers.dense(input_data, 256, activation=tf.nn.relu, name='hidden1') \n",
    "    h2 = tf.layers.dense(h1, 128, activation=tf.nn.relu, name='hidden2') \n",
    "    h3 = tf.layers.dense(h2, 64, activation=tf.nn.relu, name='hidden3')\n",
    "    out = tf.layers.dense(h3, 2, name='output')\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true, logits=out), name='loss')\n",
    "    \n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(out), 1), tf.argmax(y_true, 1))\n",
    "    compute_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "with tf.name_scope('opt'):\n",
    "    update = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
    "    \n",
    "saver = tf.train.Saver()\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100\n",
    "bs = 32\n",
    "\n",
    "train_loss_epoch, train_acc_epoch = [], []\n",
    "test_loss_epoch, test_acc_epoch = [], []\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "best_loss = 1.\n",
    "patience = 5\n",
    "count = 0\n",
    "\n",
    "for i in tqdm_notebook(range(epoch)):\n",
    "    \n",
    "#     training part\n",
    "    train_loss_batch, train_acc_batch = [], []\n",
    "    \n",
    "    total_batch = len(X_train) // bs\n",
    "    \n",
    "    for j in range(total_batch):\n",
    "        \n",
    "        X_batch = X_train[j*bs : (j+1)*bs]\n",
    "        y_batch = y_train[j*bs : (j+1)*bs]\n",
    "        batch_loss, batch_acc, _ = sess.run([loss, compute_acc, update], \n",
    "                                            feed_dict={input_data: X_batch, y_true: y_batch})\n",
    "        \n",
    "        train_loss_batch.append(batch_loss)\n",
    "        train_acc_batch.append(batch_acc)\n",
    "        \n",
    "    train_loss_epoch.append(np.mean(train_loss_batch))\n",
    "    train_acc_epoch.append(np.mean(train_acc_batch))\n",
    "    \n",
    "#     testing part\n",
    "    batch_loss, batch_acc = sess.run([loss, compute_acc], \n",
    "                                     feed_dict={input_data: X_test, y_true: y_test})\n",
    "\n",
    "    test_loss_epoch.append(batch_loss)\n",
    "    test_acc_epoch.append(batch_acc)\n",
    "    \n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    \n",
    "    if i%5 == 0:\n",
    "        print('step: {:2d}, train loss: {:.3f}, train acc: {:.3f}, test loss: {:.3f}, test acc: {:.3f}'\n",
    "             .format(i, train_loss_epoch[i], train_acc_epoch[i], test_loss_epoch[i], test_acc_epoch[i]))\n",
    "        \n",
    "    if batch_loss < best_loss:\n",
    "        best_loss = batch_loss\n",
    "        saver.save(sess, './cd_class/bestweight.ckpt', global_step=i)\n",
    "        count = 0  \n",
    "    else:\n",
    "        count += 1\n",
    "    \n",
    "    if count >= patience:\n",
    "        print(\"The model didn't improve for {} rounds, break it!\".format(patience))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.restore(sess, tf.train.latest_checkpoint('./cd_class'))  \n",
    "# saver.restore(sess, './cd_class/bestweight.ckpt-XX')\n",
    "print(sess.run(loss, feed_dict={input_data: X_test, y_true: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_epoch, 'b', label='train')\n",
    "plt.plot(test_loss_epoch, 'r', label='test')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_acc_epoch, 'b', label='train')\n",
    "plt.plot(test_acc_epoch, 'r', label='test')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$ L(x, y) = L'(x, y) + \\lambda \\sum_{i=1}^n \\theta_{i}^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope('placeholder'):\n",
    "    input_data = tf.placeholder(tf.float32, shape=[None, picsize*picsize], name='X')\n",
    "    y_true = tf.placeholder(tf.float32, shape=[None, 2], name='y')\n",
    "    l2 = tf.placeholder(tf.float32, shape=[], name='l2_regulizer')\n",
    "    \n",
    "with tf.variable_scope('network'):\n",
    "    h1 = tf.layers.dense(input_data, 256, activation=tf.nn.relu, name='hidden1',\n",
    "                         kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=l2)) \n",
    "    h2 = tf.layers.dense(h1, 128, activation=tf.nn.relu, name='hidden2',\n",
    "                         kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=l2)) \n",
    "    h3 = tf.layers.dense(h2, 64, activation=tf.nn.relu, name='hidden3',\n",
    "                         kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=l2))\n",
    "    out = tf.layers.dense(h3, 2, name='output',\n",
    "                          kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=l2))\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    cross_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true, logits=out), \n",
    "                                name='cross_entropy')\n",
    "    reg = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = cross_loss + tf.reduce_sum(reg)\n",
    "    \n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(out), 1), tf.argmax(y_true, 1))\n",
    "    compute_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "with tf.name_scope('opt'):\n",
    "    update = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {}\n",
    "for l2_reg in [0, 0.1, 0.01, 0.001]:\n",
    "    \n",
    "    epoch = 100\n",
    "    bs = 32\n",
    "\n",
    "    train_loss_epoch, train_acc_epoch = [], []\n",
    "    test_loss_epoch, test_acc_epoch = [], []\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in tqdm_notebook(range(epoch)):\n",
    "\n",
    "    #     training part\n",
    "        train_loss_batch, train_acc_batch = [], []\n",
    "\n",
    "        total_batch = len(X_train) // bs\n",
    "\n",
    "        for j in range(total_batch):\n",
    "\n",
    "            X_batch = X_train[j*bs : (j+1)*bs]\n",
    "            y_batch = y_train[j*bs : (j+1)*bs]\n",
    "            batch_loss, batch_acc, _ = sess.run([loss, compute_acc, update], \n",
    "                                                feed_dict={input_data: X_batch, y_true: y_batch, l2: l2_reg})\n",
    "\n",
    "            train_loss_batch.append(batch_loss)\n",
    "            train_acc_batch.append(batch_acc)\n",
    "\n",
    "        train_loss_epoch.append(np.mean(train_loss_batch))\n",
    "        train_acc_epoch.append(np.mean(train_acc_batch))\n",
    "\n",
    "    #     testing part\n",
    "        batch_loss, batch_acc = sess.run([loss, compute_acc], \n",
    "                                         feed_dict={input_data: X_test, y_true: y_test, l2: l2_reg})\n",
    "\n",
    "        test_loss_epoch.append(batch_loss)\n",
    "        test_acc_epoch.append(batch_acc)\n",
    "\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        \n",
    "    sess.close()\n",
    "    \n",
    "    history[l2_reg] = [train_loss_epoch, train_acc_epoch, test_loss_epoch, test_acc_epoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "for ax, key in zip(axes, history.keys()):\n",
    "    ax.plot(history[key][0], 'b', label='train')\n",
    "    ax.plot(history[key][2], 'r', label='test')\n",
    "    ax.set_title('Loss, $\\lambda = {}$'.format(key))\n",
    "    ax.legend()\n",
    "    \n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "for ax, key in zip(axes, history.keys()):\n",
    "    ax.plot(history[key][1], 'b', label='train')\n",
    "    ax.plot(history[key][3], 'r', label='test')\n",
    "    ax.set_title('Accuracy, $\\lambda = {}$'.format(key))\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/dropout.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope('placeholder'):\n",
    "    input_data = tf.placeholder(tf.float32, shape=[None, picsize*picsize], name='X')\n",
    "    y_true = tf.placeholder(tf.float32, shape=[None, 2], name='y')\n",
    "    dropout = tf.placeholder(tf.float32, shape=[], name='dropout')\n",
    "    training = tf.placeholder(tf.bool, name='training')\n",
    "    \n",
    "with tf.variable_scope('network'):\n",
    "    input_drop = tf.layers.dropout(inputs=input_data, rate=dropout, training=training, name='input_drop')\n",
    "    \n",
    "    h1 = tf.layers.dense(input_drop, 256, activation=tf.nn.relu, name='hidden1')\n",
    "    h1 = tf.layers.dropout(h1, rate=dropout, training=training, name='h1_drop')\n",
    "    \n",
    "    h2 = tf.layers.dense(h1, 128, activation=tf.nn.relu, name='hidden2') \n",
    "    h2 = tf.layers.dropout(h2, rate=dropout, training=training, name='h2_drop')\n",
    "    \n",
    "    h3 = tf.layers.dense(h2, 64, activation=tf.nn.relu, name='hidden3')\n",
    "    \n",
    "    out = tf.layers.dense(h3, 2, name='output')\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true, logits=out), name='loss')\n",
    "    \n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(out), 1), tf.argmax(y_true, 1))\n",
    "    compute_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "with tf.name_scope('opt'):\n",
    "    update = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {}\n",
    "for droprate in [0, 0.25, 0.5, 0.75]:\n",
    "    \n",
    "    epoch = 100\n",
    "    bs = 32\n",
    "\n",
    "    train_loss_epoch, train_acc_epoch = [], []\n",
    "    test_loss_epoch, test_acc_epoch = [], []\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in tqdm_notebook(range(epoch)):\n",
    "\n",
    "    #     training part\n",
    "        train_loss_batch, train_acc_batch = [], []\n",
    "\n",
    "        total_batch = len(X_train) // bs\n",
    "\n",
    "        for j in range(total_batch):\n",
    "\n",
    "            X_batch = X_train[j*bs : (j+1)*bs]\n",
    "            y_batch = y_train[j*bs : (j+1)*bs]\n",
    "            batch_loss, batch_acc, _ = sess.run([loss, compute_acc, update], \n",
    "                                                feed_dict={input_data: X_batch, y_true: y_batch, \n",
    "                                                           dropout: droprate, training: True})\n",
    "\n",
    "            train_loss_batch.append(batch_loss)\n",
    "            train_acc_batch.append(batch_acc)\n",
    "\n",
    "        train_loss_epoch.append(np.mean(train_loss_batch))\n",
    "        train_acc_epoch.append(np.mean(train_acc_batch))\n",
    "\n",
    "    #     testing part\n",
    "        batch_loss, batch_acc = sess.run([loss, compute_acc], \n",
    "                                         feed_dict={input_data: X_test, y_true: y_test, \n",
    "                                                    dropout: droprate, training: False})\n",
    "\n",
    "        test_loss_epoch.append(batch_loss)\n",
    "        test_acc_epoch.append(batch_acc)\n",
    "\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        \n",
    "    sess.close()\n",
    "    \n",
    "    history[droprate] = [train_loss_epoch, train_acc_epoch, test_loss_epoch, test_acc_epoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "for ax, key in zip(axes, history.keys()):\n",
    "    ax.plot(history[key][0], 'b', label='train')\n",
    "    ax.plot(history[key][2], 'r', label='test')\n",
    "    ax.set_title('Loss, dropout rate: {}'.format(key))\n",
    "    ax.legend()\n",
    "    \n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "for ax, key in zip(axes, history.keys()):\n",
    "    ax.plot(history[key][1], 'b', label='train')\n",
    "    ax.plot(history[key][3], 'r', label='test')\n",
    "    ax.set_title('Accuracy, dropout rate: {}'.format(key))\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
