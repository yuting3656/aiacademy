{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here, we decide to learn tensorflow from a simple example directly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General writing flow\n",
    "1. import required libraries\n",
    "2. load data and do some data pre-processing\n",
    "3. split your data into training and validation set\n",
    "4. build the network\n",
    "5. train the model and record/monitoring the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import required libries and set some parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparameter\n",
    "batch_size = 32\n",
    "epochs = 200\n",
    "lr = 0.01\n",
    "train_ratio = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data and do some pre-processing\n",
    "We use MNIST HERE (with sklearn 8x8 version rather than use tensorflow 28x28 version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "x_, y_ = digits.data, digits.target\n",
    "\n",
    "# min-max normalization\n",
    "x_ = x_ / x_.max() \n",
    "\n",
    "# one hot encoding\n",
    "y_one_hot = np.zeros((len(y_), 10))  \n",
    "y_one_hot[np.arange(len(y_)), y_] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split your data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_, \n",
    "                                                    y_one_hot, \n",
    "                                                    test_size=0.05, \n",
    "                                                    stratify=y_)\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, \n",
    "                                                      y_train, \n",
    "                                                      test_size=1.0 - train_ratio,\n",
    "                                                      stratify=y_train.argmax(axis=1))\n",
    "\n",
    "print(\"training set data dimension\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\"-----------\")\n",
    "print(\"training set: {}\".format(len(x_train)))\n",
    "print(\"validation set: {}\".format(len(x_valid)))\n",
    "print(\"testing set: {}\".format(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[0].reshape(8, 8), cmap='gray')\n",
    "plt.show()\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a.Build the network with low-level tensor elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pics/ex4-1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _What we want to train are those lines(weights), not those neurons._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    x_input = tf.placeholder(shape=(None, 64), name='x_input', dtype=tf.float32)\n",
    "    y_out = tf.placeholder(shape=(None, 10), name='y_label', dtype=tf.float32)\n",
    "\n",
    "with tf.variable_scope('hidden_layer'):\n",
    "    w1 = tf.Variable(tf.truncated_normal(shape=[64, 25], stddev=0.1),\n",
    "                     name='weight1',\n",
    "                     dtype=tf.float32)\n",
    "    b1 = tf.Variable(tf.constant(0.0, shape=[25]),\n",
    "                     name='bias1', \n",
    "                     dtype=tf.float32)\n",
    "    z1 = tf.add(tf.matmul(x_input, w1), b1)  # (None, 64)Ã—(64, 25)+(None, 25) = (None, 25)\n",
    "    a1 = tf.nn.relu(z1, name='h1_out')\n",
    "    \n",
    "with tf.variable_scope('output_layer'):\n",
    "    w2 = tf.Variable(tf.truncated_normal(shape=[25, 10], stddev=0.1),\n",
    "                     name='weight2',\n",
    "                     dtype=tf.float32)\n",
    "    b2 = tf.Variable(tf.constant(0.0, shape=[10]),\n",
    "                     name='bias2', \n",
    "                     dtype=tf.float32)\n",
    "    output = tf.add(tf.matmul(a1, w2), b2, name='output')\n",
    "\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=y_out), name='loss')\n",
    "    \n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(output), 1), tf.argmax(y_out, 1)) \n",
    "    compute_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) \n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5a.Train the model and record the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a session and train the model\n",
    "train_loss_epoch, valid_loss_epoch = [], []\n",
    "train_acc_epoch, valid_acc_epoch = [], []\n",
    "\n",
    "sess = tf.Session()\n",
    "    \n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in tqdm_notebook(range(epochs)):\n",
    "\n",
    "    total_batch = len(x_train) // batch_size \n",
    "    train_loss_batch, train_acc_batch = [], []\n",
    "\n",
    "#     training\n",
    "    for j in range(total_batch):\n",
    "\n",
    "        batch_idx_start = j * batch_size\n",
    "        batch_idx_stop = (j+1) * batch_size\n",
    "\n",
    "        x_batch = x_train[batch_idx_start : batch_idx_stop]  # xbatch = xtrain[0:32], xbatch = xtrain[32:64], and so on...\n",
    "        y_batch = y_train[batch_idx_start : batch_idx_stop]\n",
    "\n",
    "        batch_loss, batch_acc, _ = sess.run([loss, compute_acc, train_step], \n",
    "                                            feed_dict={x_input: x_batch, y_out: y_batch})\n",
    "\n",
    "        train_loss_batch.append(batch_loss) \n",
    "        train_acc_batch.append(batch_acc)  \n",
    "\n",
    "#     validation\n",
    "    valid_acc, valid_loss = sess.run([compute_acc, loss],\n",
    "                                     feed_dict={x_input: x_valid, y_out : y_valid})\n",
    "    \n",
    "#     collect loss and accuracy\n",
    "    train_loss_epoch.append(np.mean(train_loss_batch)) \n",
    "    train_acc_epoch.append(np.mean(train_acc_batch))\n",
    "    valid_loss_epoch.append(valid_loss) \n",
    "    valid_acc_epoch.append(valid_acc) \n",
    "\n",
    "    x_train, y_train = shuffle(x_train, y_train)\n",
    "    \n",
    "print('--- training done ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.plot(train_loss_epoch, 'b', label='train')\n",
    "plt.plot(valid_loss_epoch, 'r', label='valid')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_acc_epoch, 'b', label='train')\n",
    "plt.plot(valid_acc_epoch, 'r', label='valid')\n",
    "plt.legend(loc=4)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, test_loss = sess.run([compute_acc, loss],\n",
    "                                feed_dict = {x_input: x_test, y_out : y_test})\n",
    "\n",
    "print('testing accuracy: {:.2f}'.format(test_acc))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. Build the network with \"layer\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    x_input = tf.placeholder(shape=(None, 64), \n",
    "                             name='x_input',\n",
    "                             dtype=tf.float32)\n",
    "    y_out = tf.placeholder(shape=(None, 10), \n",
    "                           name='y_label',\n",
    "                           dtype=tf.float32)\n",
    "\n",
    "with tf.variable_scope('hidden_layer'):\n",
    "    x_h1 = tf.layers.dense(inputs=x_input, units=25, activation=tf.nn.relu)\n",
    "\n",
    "with tf.variable_scope('output_layer'):\n",
    "    output = tf.layers.dense(x_h1, 10, name='output')\n",
    "\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=y_out), name='loss')\n",
    "    \n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(output), 1), tf.argmax(y_out, 1))\n",
    "    compute_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b.Train the model and record the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a session and train the model\n",
    "train_loss_epoch, valid_loss_epoch = [], []\n",
    "train_acc_epoch, valid_acc_epoch = [], []\n",
    "\n",
    "sess = tf.Session()\n",
    "    \n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in tqdm_notebook(range(epochs)):\n",
    "\n",
    "    total_batch = len(x_train) // batch_size \n",
    "    train_loss_in_batch, train_acc_in_batch = [], []\n",
    "\n",
    "    for j in range(total_batch):\n",
    "\n",
    "        batch_idx_start = j * batch_size\n",
    "        batch_idx_stop = (j+1) * batch_size\n",
    "\n",
    "        x_batch = x_train[batch_idx_start : batch_idx_stop] \n",
    "        y_batch = y_train[batch_idx_start : batch_idx_stop]\n",
    "\n",
    "        this_loss, this_acc, _ = sess.run([loss, compute_acc, train_step], \n",
    "                                          feed_dict={x_input: x_batch, y_out: y_batch})\n",
    "\n",
    "        train_loss_in_batch.append(this_loss) \n",
    "        train_acc_in_batch.append(this_acc)  \n",
    "\n",
    "\n",
    "    valid_acc, valid_loss = sess.run([compute_acc, loss],\n",
    "                                     feed_dict={x_input: x_valid, y_out : y_valid})\n",
    "    \n",
    "    valid_loss_epoch.append(valid_loss) \n",
    "    valid_acc_epoch.append(valid_acc)   \n",
    "    train_loss_epoch.append(np.mean(train_loss_in_batch)) \n",
    "    train_acc_epoch.append(np.mean(train_acc_in_batch))  \n",
    "\n",
    "    x_train, y_train = shuffle(x_train, y_train)\n",
    "\n",
    "\n",
    "print('--- training done ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.plot(train_loss_epoch, 'b', label='train')\n",
    "plt.plot(valid_loss_epoch, 'r', label='valid')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_acc_epoch, 'b', label='train')\n",
    "plt.plot(valid_acc_epoch, 'r', label='valid')\n",
    "plt.legend(loc=4)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, test_loss = sess.run([compute_acc, loss],\n",
    "                                feed_dict={x_input: x_test, y_out : y_test})\n",
    "\n",
    "print('testing accuracy: {:.2f}'.format(test_acc))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice\n",
    "Build a neural network with 3 hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pics/ex4-2.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
