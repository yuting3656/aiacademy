{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dogs vs. Cats Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import inquired liberies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cat_dog_df.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hint: use cv2 to read jpg files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic = cv2.imread(df.file_name[0], cv2.IMREAD_GRAYSCALE)\n",
    "print(pic.shape)\n",
    "plt.imshow(pic, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split data and do preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df.type)\n",
    "\n",
    "def df_to_data(df, picsize):\n",
    "    img_ls = []\n",
    "    label_ls = []\n",
    "\n",
    "    for file_name, label in tqdm_notebook(zip(df.file_name, df.type)):\n",
    "        img = cv2.imread(file_name, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (picsize, picsize))\n",
    "        img_ls.append(img)\n",
    "        \n",
    "        onehot = np.zeros(2)  \n",
    "        onehot[label] = 1\n",
    "        label_ls.append(onehot)\n",
    "\n",
    "    x_data = np.array(img_ls).reshape(len(df), -1)  # flatten image data\n",
    "    x_data = x_data / 255.  # normalization\n",
    "    y_data = np.array(label_ls)\n",
    "    return x_data, y_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picsize = 64\n",
    "X_train, y_train = df_to_data(train_df, picsize)\n",
    "X_test, y_test = df_to_data(test_df, picsize)\n",
    "\n",
    "print('size of training data:', X_train.shape, y_train.shape)\n",
    "print('size of testing data:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope('placeholder'):\n",
    "    input_data = tf.placeholder(tf.float32, shape=[None, picsize*picsize], name='X')\n",
    "    y_true = tf.placeholder(tf.float32, shape=[None, 2], name='y')\n",
    "    \n",
    "with tf.variable_scope('network'):\n",
    "    h1 = tf.layers.dense(input_data, 256, activation=tf.nn.sigmoid, name='hidden1')  # try to change the activation function\n",
    "    h2 = tf.layers.dense(h1, 128, activation=tf.nn.sigmoid, name='hidden2') \n",
    "    h3 = tf.layers.dense(h2, 64, activation=tf.nn.sigmoid, name='hidden3')\n",
    "    out = tf.layers.dense(h3, 2, name='output')\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true, logits=out), name='loss')\n",
    "    \n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(out), 1), tf.argmax(y_true, 1))\n",
    "    compute_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "with tf.name_scope('opt'):\n",
    "    update = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss) # try to change the optimuizer and learning rate\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_sum = 0\n",
    "for tensor in tf.global_variables(scope='network'):\n",
    "    if 'Adam' not in tensor.name:\n",
    "        var_sum += np.product(tensor.shape.as_list())\n",
    "    \n",
    "print('the total number of weights is', var_sum)  # 4096*256 + 256 + 256*128 + 128 + 128*64 + 64 + 64*2 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100\n",
    "bs = 32\n",
    "\n",
    "train_loss_epoch, train_acc_epoch = [], []\n",
    "test_loss_epoch, test_acc_epoch = [], []\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in tqdm_notebook(range(epoch)):\n",
    "    \n",
    "#     training part\n",
    "    train_loss_batch, train_acc_batch = [], []\n",
    "    \n",
    "    total_batch = len(X_train) // bs\n",
    "    \n",
    "    for j in range(total_batch):\n",
    "        \n",
    "        X_batch = X_train[j*bs : (j+1)*bs]\n",
    "        y_batch = y_train[j*bs : (j+1)*bs]\n",
    "        batch_loss, batch_acc, _ = sess.run([loss, compute_acc, update], \n",
    "                                            feed_dict={input_data: X_batch, y_true: y_batch})\n",
    "        \n",
    "        train_loss_batch.append(batch_loss)\n",
    "        train_acc_batch.append(batch_acc)\n",
    "        \n",
    "    train_loss_epoch.append(np.mean(train_loss_batch))\n",
    "    train_acc_epoch.append(np.mean(train_acc_batch))\n",
    "    \n",
    "#     testing part\n",
    "    batch_loss, batch_acc = sess.run([loss, compute_acc], \n",
    "                                    feed_dict={input_data: X_test, y_true: y_test})\n",
    "\n",
    "    test_loss_epoch.append(batch_loss)\n",
    "    test_acc_epoch.append(batch_acc)\n",
    "    \n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    \n",
    "    if i%5 == 0:\n",
    "        print('step: {:2d}, train loss: {:.3f}, train acc: {:.3f}, test loss: {:.3f}, test acc: {:.3f}'\n",
    "             .format(i, train_loss_epoch[i], train_acc_epoch[i], test_loss_epoch[i], test_acc_epoch[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_epoch, 'b', label='train')\n",
    "plt.plot(test_loss_epoch, 'r', label='test')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_acc_epoch, 'b', label='train')\n",
    "plt.plot(test_acc_epoch, 'r', label='test')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice\n",
    "Overfitting? Try to remove some hidden layers or neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
