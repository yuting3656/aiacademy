{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid: \n",
    "<hr>\n",
    "Sigmoid (AKA logistic regression) 是一個用來把資料壓到 0 跟 1 之間的一個 function<br>\n",
    "雖然 Sigmoid 非常好用 <br>\n",
    "不過之後的課程也會提到 Sigmoid 的一些壞處<br>\n",
    "請各位另用以下的算式寫出一個 Sigmoid 的 function<br>\n",
    "\n",
    "\n",
    "### $ Z(x_i) = \\frac{1}{1+e^{-x_i}}$\n",
    "<hr>\n",
    "\n",
    ">提示：可以使用 np.exp 來算 e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "pb2Y8rZgthxd"
   },
   "outputs": [],
   "source": [
    "# define activation: sigmoid\n",
    "def sigmoid(X):\n",
    "    output = 1 / (1 + np.exp(-X)) \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples\n",
    "X = np.arange(5)\n",
    "sigmoid(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code should return <br>\n",
    "array([0.5       , 0.73105858, 0.88079708, 0.95257413, 0.98201379])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Gradient: \n",
    "<hr>\n",
    "因為後面要倒推回來<br>\n",
    "在微分的過程中Sigmoid也需要作微分<br>\n",
    "雖然很想叫你們自己推<br>\n",
    "但是這樣好像太殘忍了<br>\n",
    "所以下面給你們sigmoid gradient的式子<br>\n",
    "\n",
    "### $ \\frac{d}{dx} Z(x_i) = Z(x_i) \\times (1-Z(x_i))$\n",
    "這邊的Z代表Sigmoid<br>\n",
    "如果你對怎麼推導的有興趣<br>\n",
    "以下附上式子推導的過程<br>\n",
    "[數學推導過程](http://www.ai.mit.edu/courses/6.892/lecture8-html/sld015.htm)\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(X):\n",
    "    output = sigmoid(X)*(1-sigmoid(X))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples\n",
    "X = np.arange(5)\n",
    "sigmoid_gradient(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code should return <br>\n",
    "array([0.25      , 0.19661193, 0.10499359, 0.04517666, 0.01766271])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TdKQ96zhthxh"
   },
   "source": [
    "## Softmax:\n",
    "<hr>\n",
    "Softmax (AKA normalized exponential function) 與 Sigmoid 有點類似 <br>\n",
    "不過 Softmax 是針對一整組數字做壓縮 <br>\n",
    "而 Sigmoid 是直接對單一的數值做壓縮\n",
    "請各位另用以下的算式寫出一個 Softmax 的 function<br>\n",
    "\n",
    "\n",
    "### $\\sigma(\\mathbf{x})_j = \\frac{e^{x_j}}{\\sum_{i=1}^K e^{x_i}}\\,for\\,j\\,\\in\\,1,\\,\\ldots\\,,\\,K$ \n",
    "\n",
    "For example:<br>\n",
    "如果我們有一組數列 $1,\\,3,\\,5$ <br>\n",
    "Softmax 會回傳 $0.016,\\,0.117,\\,0.867$\n",
    "\n",
    "<hr>\n",
    "\n",
    ">提示：python 會自動 broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "81cUlLE4thxi"
   },
   "outputs": [],
   "source": [
    "# define activation: softmax\n",
    "def softmax(X): \n",
    "    return np.exp(X) / np.sum(np.exp(X), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples\n",
    "X = np.array([np.arange(5)])\n",
    "softmax(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code should return array([[0.01165623, 0.03168492, 0.08612854, 0.23412166, 0.63640865]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross entropy (Multiclass)\n",
    "<hr>\n",
    "Definition: Cross-entropy loss, or log loss, measures the performance of a classification model<br>\n",
    "whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted<br>\n",
    "probability diverges from the actual label. So predicting a probability of .012 when the actual<br>\n",
    "observation label is 1 would be bad and result in a high loss value. A perfect model would have a <br>\n",
    "log loss of 0.<br><br>\n",
    "Cross entropy 的算式如下：<br>\n",
    "\n",
    "### $H(p,q)=-\\displaystyle\\sum _{i=1}^m  p(x_i)\\,\\log q(x_i).\\!$\n",
    "在這邊 p 代表的是我們的實際值，q 代表的是我們的預測值，$x_i$ 是我們的samples，m 是sample總數。<br>\n",
    "如果還是不太清楚的話以下提供一個例子： <br><br>\n",
    ">假設我們的第一個Y值(實際值)為 \\[1, 0, 0\\]，預測值為 \\[0.7, 0.2, 0.1\\] 的話\n",
    ">#### $p(x_{0})\\,\\log q(x_{0})=1\\times \\log 0.7 + 0\\times \\log 0.2 + 0\\times \\log 0.1 \\approx -0.357$\n",
    "<!---\n",
    "1. 假設我們的實際值為0，預測值為0.1的話\n",
    "#### $-\\sum_{x_i}p(x_{i})\\,\\log q(x_{i})=0\\times \\log 0.1 +(1-0) \\times \\log (1-0.1) = 0.105$\n",
    "--->\n",
    "這邊會是負數是因為log 1 到 0 之間的數值的話都會回傳負值<br>\n",
    "但是沒關係  <br>\n",
    "我們做完sum之後會在乘上 -1  <br><br>\n",
    "請大家用這個算式自己寫寫看一個cross entropy 的function <br>\n",
    "<hr>\n",
    "\n",
    "><b>提示：</b> <br>\n",
    ">由於log(0)會出現錯誤，所以我們通常會加一個epsilon，通常會設定為1e-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(p, q):\n",
    "    epsilon = 1e-15\n",
    "    H = 0\n",
    "    for i in range(len(p)):\n",
    "        H += -p[i]*np.log(q[i]+epsilon)\n",
    "        \n",
    "    H = H.sum()/p.shape[0]\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "p = np.array([[1,0,0]])\n",
    "q = np.array([[0.7, 0.2, 0.1]])\n",
    "cross_entropy(p,q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code should return 0.356674943938731 in this example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "<hr>\n",
    "如果你有認真在做上一個練習的話(拜託說有，說沒有我會難過) <br>\n",
    "你可能會發現我們的 class 是用 0，1，跟 2 在做區分的<br>\n",
    "這樣的分法叫做 Label Encoding<br>\n",
    "雖然這個方法很方便<br>\n",
    "但是由於數值大小不同有時候會造成一些偏差<br>\n",
    "所以這邊我們給大家介紹另一種分類的方法<br>\n",
    "叫做 One Hot Encoding<br>\n",
    "<br>\n",
    "One Hot Encoding 做的事情就是把一個有 N 個不同種類的 array<br>\n",
    "變成一個有 N 個 Column 的矩陣<br>\n",
    "然後每一個 Column 都只會出現 1 或是 0<br>\n",
    "每一個 Column 也都對應到 N 個種類中的其中一種<br>\n",
    "雖然網路上有很多 package 在做這件事了<br>\n",
    "但到現在你們可能已經發現我不太喜歡用 package 了<br>\n",
    "所以我們就來自己寫一個吧!<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    ">這邊可以偷用 np.sort <br>\n",
    "> 因為我覺得這個 function 很基本 <br>\n",
    ">所以勉強給大家直接用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(array):\n",
    "    \n",
    "    sorted_array = np.sort(array)\n",
    "    count = 1\n",
    "    unique = [sorted_array[0]]\n",
    "    \n",
    "    temp = sorted_array[0]\n",
    "    for i in range(len(array)):\n",
    "        if sorted_array[i] != temp:\n",
    "            count += 1\n",
    "            temp = sorted_array[i]\n",
    "            unique.append(temp)\n",
    "            \n",
    "    eye = np.zeros((len(unique), len(unique)))\n",
    "    for i in range(len(unique)):\n",
    "        eye[i, i] = 1\n",
    "    \n",
    "    for i in range(len(array)):\n",
    "        for j in range(len(unique)):\n",
    "            if array[i] == unique[j]:\n",
    "                array[i] = j\n",
    "                break\n",
    "                \n",
    "    result = eye[array]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example.\n",
    "array = np.array([1,2,3,2])\n",
    "one_hot_encoding(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code should return<br>\n",
    "[1., 0., 0.],<br>\n",
    "[0., 1., 0.],<br>\n",
    "[0., 0., 1.],<br>\n",
    "[0., 1., 0.]<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "<hr>\n",
    "看了這麼多邪惡的數學 <br>\n",
    "現在終於要開始寫比較善良的神經網路了<br>\n",
    "不過我們這邊還是要定義一些變數<br>\n",
    "我們先來看一下我們今天要做的Neural Network的架構\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/0*hzIQ5Fs-g8iBpVWq.jpg)<br>\n",
    "[image source](https://towardsdatascience.com/coding-neural-network-forward-propagation-and-backpropagtion-ccf8cf369f76) <br> <br>\n",
    "\n",
    "以上是我們今天要寫的神經網路的架構<br>\n",
    "我們今天要做的流程是<br>\n",
    "Forward\n",
    "1. $layer_1 = X \\times weight_1 $\n",
    "2. $activation_1 = activation(z_1)$\n",
    "3. $layer_2 = a_1 \\times weight_2$\n",
    "4. $prediction = activationo(z_2)$<br>\n",
    "\n",
    "Loss Function<br>\n",
    "\n",
    "1. $loss = loss\\,function(true\\,value,\\,prediction)$<br>\n",
    "\n",
    "Backward<br>\n",
    "\n",
    "1. $derivative\\,of\\,layer_2 = prediction - true\\,value$\n",
    "2. $derivative\\,of\\,weight_2  = activation_1\\times derivative\\,of\\,layer_2$\n",
    "3. $derivative\\,of\\,layer_1 = derivative\\,of\\,layer_2\\times weight_2\\times gradient\\,of\\,activation_1$\n",
    "4. $derivative\\,of\\,weight_1  = X\\times derivative\\,of\\,layer_1$\n",
    "\n",
    "看到這邊你應該會注意到兩件事情<br>\n",
    "第一件事情是我們這邊沒有加上bias<br>\n",
    "這邊沒有加上bias是因為會讓model變得比較複雜<br>\n",
    "為了簡化model所以沒有加上去<br>\n",
    "第二件事情是Backward的第一步<br>\n",
    "為什麼我們沒有做 softmax 和 cross entropy 的微分<br>\n",
    "然後就模明奇妙用 prediction 減掉 true value<br>\n",
    "其實是因為這個就是 cross entropy 加上 softmax 後的微分<br>\n",
    "以下提供美麗的數學推導<br>\n",
    "[Beautiful Math](https://deepnotes.io/softmax-crossentropy)\n",
    "<!---\n",
    "首先看到最左邊的input layer <br>\n",
    "這一層是用來放我們整理好的資料 <br>\n",
    "它們的呈現方式通常會是一個很大的Matrix <br>\n",
    "只是這邊為了方便呈現 <br>\n",
    "我們把Matrix裡面的每一列都用一個圓圈表示<br>\n",
    "<br>\n",
    "接下來就進到我們的第一層神經網路<br>\n",
    "我們所有的資料都會進入到第一層神經網路裡面的每一個神經元<br>\n",
    "--->\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_net(X, Y, W1, W2):\n",
    "    # Forward\n",
    "    z1 = np.matmul(X, W1) \n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.matmul(a1, W2) \n",
    "    out = softmax(z2)\n",
    "    J = cross_entropy(Y, out)\n",
    "    # Backward\n",
    "    d2 = out - Y\n",
    "    dW2 = np.matmul(a1.T, d2)\n",
    "    d1 = np.matmul(d2, (W2.T))*sigmoid_gradient(a1)\n",
    "    dW1 = np.matmul(X.T, d1)\n",
    "    \n",
    "    return J, dW1, dW2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data\n",
    "<hr>\n",
    "終於到了要來測試我們的NN有多好的時候了<br>\n",
    "但首先我們跟上一份一樣<br>\n",
    "先偷偷 import sklearn 來下載 iris 的資料<br>\n",
    "如果你問我為什麼又要用 iris 的資料<br>\n",
    "那我... 也回答不出來<br>\n",
    "反正這麼乾淨的資料就用嘛 <br>\n",
    "不要想那麼多<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "Y = one_hot_encoding(Y)\n",
    "names = iris.target_names\n",
    "\n",
    "X_train = np.vstack([X[0:40], X[50:90], X[100:140]])\n",
    "X_valid = np.vstack([X[40:45], X[90:95], X[140:145]])\n",
    "X_test = np.vstack([X[45:50], X[95:100], X[145:150]])\n",
    "\n",
    "Y_train = np.vstack([Y[0:40], Y[50:90], Y[100:140]])\n",
    "Y_valid = np.vstack([Y[40:45], Y[90:95], Y[140:145]])\n",
    "Y_test = np.vstack([Y[45:50], Y[95:100], Y[145:150]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to train\n",
    "<hr>\n",
    "這邊有很多參數可以調<br>\n",
    "唯一需要特別注意的是<br>\n",
    "如果你要想要改變 weight<br>\n",
    "記得 W1 後面要跟 W2 前面一樣<br>\n",
    "原因自己想!<br>\n",
    "好啦因為矩陣相乘所以需要一樣<br>\n",
    "<hr>\n",
    "\n",
    ">提示：如果 Function output 三個東西可是我們只要一個東西的話<br>\n",
    ">     我們可以用底線 _ 來忽略\n",
    "```python \n",
    "J_valid, _, _ = two_layer_net......\n",
    "``` \n",
    "><br>或是我們可以直接用中括弧跟數字來代表<br>\n",
    "```python\n",
    "J_valid = two_layer_net(......)[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 1000\n",
    "alpha = 0.01\n",
    "history_train = np.zeros((iteration, 1))\n",
    "history_valid = np.zeros((iteration, 1))\n",
    "\n",
    "np.random.seed(37)\n",
    "W1 = np.random.randn(4,10)\n",
    "W2 = np.random.randn(10,3)\n",
    "\n",
    "for i in range(iteration):\n",
    "    J_train, dW1, dW2 = two_layer_net(X_train, Y_train, W1, W2)\n",
    "    J_valid, _, _ = two_layer_net(X_valid, Y_valid, W1, W2)\n",
    "    W1 -= alpha*dW1\n",
    "    #b1 -= alpha*db1\n",
    "    W2 -= alpha*dW2\n",
    "    #b2 -= alpha*db2\n",
    "    \n",
    "    history_train[i] = J_train\n",
    "    history_valid[i] = J_valid\n",
    "    if (i+1)%50 == 0:\n",
    "        print('The training loss of the', i+1, 'epoch is', history_train[i][0].round(4), ', ', end='')\n",
    "        print('The validation loss of the', i+1, 'epoch is', history_valid[i][0].round(4))\n",
    "        \n",
    "print('\\nThe loss of our testing set is ', two_layer_net(X_test, Y_test, W1, W2)[0].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
